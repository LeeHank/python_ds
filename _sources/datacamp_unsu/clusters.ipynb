{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toy-example (iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 出動 iris 資料集吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  Species\n",
       "0              5.1           3.5            1.4           0.2      0.0\n",
       "1              4.9           3.0            1.4           0.2      0.0\n",
       "2              4.7           3.2            1.3           0.2      0.0\n",
       "3              4.6           3.1            1.5           0.2      0.0\n",
       "4              5.0           3.6            1.4           0.2      0.0\n",
       "..             ...           ...            ...           ...      ...\n",
       "145            6.7           3.0            5.2           2.3      2.0\n",
       "146            6.3           2.5            5.0           1.9      2.0\n",
       "147            6.5           3.0            5.2           2.0      2.0\n",
       "148            6.2           3.4            5.4           2.3      2.0\n",
       "149            5.9           3.0            5.1           1.8      2.0\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "df_data = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm','Species'])\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "X = df_data.drop(\"Species\", axis = 1)\n",
    "kmeansModel = KMeans(n_clusters=3, # 分 3 群\n",
    "                     n_init = 10, # 起始值有10組，每組分完後，比 inertia，選最小的那組給你\n",
    "                     random_state=46, # 產出 10 組起始值時，用的 seed\n",
    "                    tol = 0.0001) # 前後兩次 iteration， centers 間的 歐式距離，小於 0.0001 就算收斂\n",
    "kmeansModel.fit(X)\n",
    "clusters_pred = kmeansModel.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 分群結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 各群的中心"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.9016129 , 2.7483871 , 4.39354839, 1.43387097],\n",
       "       [5.006     , 3.428     , 1.462     , 0.246     ],\n",
       "       [6.85      , 3.07368421, 5.74210526, 2.07105263]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeansModel.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* inertia (within-group sum of variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.85144142614601"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeansModel.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 選 k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_list = [KMeans(n_clusters=k, random_state=46).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,10), inertias);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，elbow 在 2 or 3，所以可以選 2 or 3 當分群的群數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 詳細說明文件，看 `KMeans?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "K-Means clustering.\n",
       "\n",
       "Read more in the :ref:`User Guide <k_means>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "\n",
       "n_clusters : int, default=8\n",
       "    The number of clusters to form as well as the number of\n",
       "    centroids to generate.\n",
       "\n",
       "init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n",
       "    Method for initialization:\n",
       "\n",
       "    'k-means++' : selects initial cluster centers for k-mean\n",
       "    clustering in a smart way to speed up convergence. See section\n",
       "    Notes in k_init for more details.\n",
       "\n",
       "    'random': choose `n_clusters` observations (rows) at random from data\n",
       "    for the initial centroids.\n",
       "\n",
       "    If an array is passed, it should be of shape (n_clusters, n_features)\n",
       "    and gives the initial centers.\n",
       "\n",
       "    If a callable is passed, it should take arguments X, n_clusters and a\n",
       "    random state and return an initialization.\n",
       "\n",
       "n_init : int, default=10\n",
       "    Number of time the k-means algorithm will be run with different\n",
       "    centroid seeds. The final results will be the best output of\n",
       "    n_init consecutive runs in terms of inertia.\n",
       "\n",
       "max_iter : int, default=300\n",
       "    Maximum number of iterations of the k-means algorithm for a\n",
       "    single run.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Relative tolerance with regards to Frobenius norm of the difference\n",
       "    in the cluster centers of two consecutive iterations to declare\n",
       "    convergence.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Verbosity mode.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Determines random number generation for centroid initialization. Use\n",
       "    an int to make the randomness deterministic.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "copy_x : bool, default=True\n",
       "    When pre-computing distances it is more numerically accurate to center\n",
       "    the data first. If copy_x is True (default), then the original data is\n",
       "    not modified. If False, the original data is modified, and put back\n",
       "    before the function returns, but small numerical differences may be\n",
       "    introduced by subtracting and then adding the data mean. Note that if\n",
       "    the original data is not C-contiguous, a copy will be made even if\n",
       "    copy_x is False. If the original data is sparse, but not in CSR format,\n",
       "    a copy will be made even if copy_x is False.\n",
       "\n",
       "algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n",
       "    K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
       "    The \"elkan\" variation is more efficient on data with well-defined\n",
       "    clusters, by using the triangle inequality. However it's more memory\n",
       "    intensive due to the allocation of an extra array of shape\n",
       "    (n_samples, n_clusters).\n",
       "\n",
       "    For now \"auto\" (kept for backward compatibility) chooses \"elkan\" but it\n",
       "    might change in the future for a better heuristic.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "        Added Elkan algorithm\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
       "    Coordinates of cluster centers. If the algorithm stops before fully\n",
       "    converging (see ``tol`` and ``max_iter``), these will not be\n",
       "    consistent with ``labels_``.\n",
       "\n",
       "labels_ : ndarray of shape (n_samples,)\n",
       "    Labels of each point\n",
       "\n",
       "inertia_ : float\n",
       "    Sum of squared distances of samples to their closest cluster center,\n",
       "    weighted by the sample weights if provided.\n",
       "\n",
       "n_iter_ : int\n",
       "    Number of iterations run.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "MiniBatchKMeans : Alternative online implementation that does incremental\n",
       "    updates of the centers positions using mini-batches.\n",
       "    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
       "    probably much faster than the default batch implementation.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n",
       "\n",
       "The average complexity is given by O(k n T), where n is the number of\n",
       "samples and T is the number of iteration.\n",
       "\n",
       "The worst case complexity is given by O(n^(k+2/p)) with\n",
       "n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n",
       "'How slow is the k-means method?' SoCG2006)\n",
       "\n",
       "In practice, the k-means algorithm is very fast (one of the fastest\n",
       "clustering algorithms available), but it falls in local minima. That's why\n",
       "it can be useful to restart it several times.\n",
       "\n",
       "If the algorithm stops before fully converging (because of ``tol`` or\n",
       "``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n",
       "i.e. the ``cluster_centers_`` will not be the means of the points in each\n",
       "cluster. Also, the estimator will reassign ``labels_`` after the last\n",
       "iteration to make ``labels_`` consistent with ``predict`` on the training\n",
       "set.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> from sklearn.cluster import KMeans\n",
       ">>> import numpy as np\n",
       ">>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
       "...               [10, 2], [10, 4], [10, 0]])\n",
       ">>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
       ">>> kmeans.labels_\n",
       "array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
       ">>> kmeans.predict([[0, 0], [12, 3]])\n",
       "array([1, 0], dtype=int32)\n",
       ">>> kmeans.cluster_centers_\n",
       "array([[10.,  2.],\n",
       "       [ 1.,  2.]])\n",
       "\u001b[0;31mFile:\u001b[0m           /Volumes/GoogleDrive/我的雲端硬碟/0. codepool_python/python_ds/python_ds_env/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     MiniBatchKMeans\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "KMeans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stock movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 這個應用蠻好玩的，我們可以對股票的 \"走勢\" 做分群。\n",
    "* 資料如以下： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2010-01-04</th>\n",
       "      <th>2010-01-05</th>\n",
       "      <th>2010-01-06</th>\n",
       "      <th>2010-01-07</th>\n",
       "      <th>2010-01-08</th>\n",
       "      <th>2010-01-11</th>\n",
       "      <th>2010-01-12</th>\n",
       "      <th>2010-01-13</th>\n",
       "      <th>2010-01-14</th>\n",
       "      <th>2010-01-15</th>\n",
       "      <th>...</th>\n",
       "      <th>2013-10-16</th>\n",
       "      <th>2013-10-17</th>\n",
       "      <th>2013-10-18</th>\n",
       "      <th>2013-10-21</th>\n",
       "      <th>2013-10-22</th>\n",
       "      <th>2013-10-23</th>\n",
       "      <th>2013-10-24</th>\n",
       "      <th>2013-10-25</th>\n",
       "      <th>2013-10-28</th>\n",
       "      <th>2013-10-29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Apple</th>\n",
       "      <td>0.580000</td>\n",
       "      <td>-0.220005</td>\n",
       "      <td>-3.409998</td>\n",
       "      <td>-1.170000</td>\n",
       "      <td>1.680011</td>\n",
       "      <td>-2.689994</td>\n",
       "      <td>-1.469994</td>\n",
       "      <td>2.779997</td>\n",
       "      <td>-0.680003</td>\n",
       "      <td>-4.999995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320008</td>\n",
       "      <td>4.519997</td>\n",
       "      <td>2.899987</td>\n",
       "      <td>9.590019</td>\n",
       "      <td>-6.540016</td>\n",
       "      <td>5.959976</td>\n",
       "      <td>6.910011</td>\n",
       "      <td>-5.359962</td>\n",
       "      <td>0.840019</td>\n",
       "      <td>-19.589981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIG</th>\n",
       "      <td>-0.640002</td>\n",
       "      <td>-0.650000</td>\n",
       "      <td>-0.210001</td>\n",
       "      <td>-0.420000</td>\n",
       "      <td>0.710001</td>\n",
       "      <td>-0.200001</td>\n",
       "      <td>-1.130001</td>\n",
       "      <td>0.069999</td>\n",
       "      <td>-0.119999</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919998</td>\n",
       "      <td>0.709999</td>\n",
       "      <td>0.119999</td>\n",
       "      <td>-0.480000</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>-0.279998</td>\n",
       "      <td>-0.190003</td>\n",
       "      <td>-0.040001</td>\n",
       "      <td>-0.400002</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon</th>\n",
       "      <td>-2.350006</td>\n",
       "      <td>1.260009</td>\n",
       "      <td>-2.350006</td>\n",
       "      <td>-2.009995</td>\n",
       "      <td>2.960006</td>\n",
       "      <td>-2.309997</td>\n",
       "      <td>-1.640007</td>\n",
       "      <td>1.209999</td>\n",
       "      <td>-1.790001</td>\n",
       "      <td>-2.039994</td>\n",
       "      <td>...</td>\n",
       "      <td>2.109985</td>\n",
       "      <td>3.699982</td>\n",
       "      <td>9.570008</td>\n",
       "      <td>-3.450013</td>\n",
       "      <td>4.820008</td>\n",
       "      <td>-4.079986</td>\n",
       "      <td>2.579986</td>\n",
       "      <td>4.790009</td>\n",
       "      <td>-1.760009</td>\n",
       "      <td>3.740021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American express</th>\n",
       "      <td>0.109997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260002</td>\n",
       "      <td>0.720002</td>\n",
       "      <td>0.190003</td>\n",
       "      <td>-0.270001</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.300004</td>\n",
       "      <td>0.639999</td>\n",
       "      <td>-0.130001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680001</td>\n",
       "      <td>2.290001</td>\n",
       "      <td>0.409996</td>\n",
       "      <td>-0.069999</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.069999</td>\n",
       "      <td>0.130005</td>\n",
       "      <td>1.849999</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.540001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boeing</th>\n",
       "      <td>0.459999</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>1.549999</td>\n",
       "      <td>2.690003</td>\n",
       "      <td>0.059997</td>\n",
       "      <td>-1.080002</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>0.530002</td>\n",
       "      <td>-0.709999</td>\n",
       "      <td>...</td>\n",
       "      <td>1.559997</td>\n",
       "      <td>2.480003</td>\n",
       "      <td>0.019997</td>\n",
       "      <td>-1.220001</td>\n",
       "      <td>0.480003</td>\n",
       "      <td>3.020004</td>\n",
       "      <td>-0.029999</td>\n",
       "      <td>1.940002</td>\n",
       "      <td>1.130005</td>\n",
       "      <td>0.309998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 963 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  2010-01-04  2010-01-05  2010-01-06  2010-01-07  2010-01-08  \\\n",
       "Apple               0.580000   -0.220005   -3.409998   -1.170000    1.680011   \n",
       "AIG                -0.640002   -0.650000   -0.210001   -0.420000    0.710001   \n",
       "Amazon             -2.350006    1.260009   -2.350006   -2.009995    2.960006   \n",
       "American express    0.109997    0.000000    0.260002    0.720002    0.190003   \n",
       "Boeing              0.459999    1.770000    1.549999    2.690003    0.059997   \n",
       "\n",
       "                  2010-01-11  2010-01-12  2010-01-13  2010-01-14  2010-01-15  \\\n",
       "Apple              -2.689994   -1.469994    2.779997   -0.680003   -4.999995   \n",
       "AIG                -0.200001   -1.130001    0.069999   -0.119999   -0.500000   \n",
       "Amazon             -2.309997   -1.640007    1.209999   -1.790001   -2.039994   \n",
       "American express   -0.270001    0.750000    0.300004    0.639999   -0.130001   \n",
       "Boeing             -1.080002    0.360000    0.549999    0.530002   -0.709999   \n",
       "\n",
       "                  ...  2013-10-16  2013-10-17  2013-10-18  2013-10-21  \\\n",
       "Apple             ...    0.320008    4.519997    2.899987    9.590019   \n",
       "AIG               ...    0.919998    0.709999    0.119999   -0.480000   \n",
       "Amazon            ...    2.109985    3.699982    9.570008   -3.450013   \n",
       "American express  ...    0.680001    2.290001    0.409996   -0.069999   \n",
       "Boeing            ...    1.559997    2.480003    0.019997   -1.220001   \n",
       "\n",
       "                  2013-10-22  2013-10-23  2013-10-24  2013-10-25  2013-10-28  \\\n",
       "Apple              -6.540016    5.959976    6.910011   -5.359962    0.840019   \n",
       "AIG                 0.010002   -0.279998   -0.190003   -0.040001   -0.400002   \n",
       "Amazon              4.820008   -4.079986    2.579986    4.790009   -1.760009   \n",
       "American express    0.100006    0.069999    0.130005    1.849999    0.040001   \n",
       "Boeing              0.480003    3.020004   -0.029999    1.940002    1.130005   \n",
       "\n",
       "                  2013-10-29  \n",
       "Apple             -19.589981  \n",
       "AIG                 0.660000  \n",
       "Amazon              3.740021  \n",
       "American express    0.540001  \n",
       "Boeing              0.309998  \n",
       "\n",
       "[5 rows x 963 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movements = pd.read_csv(\"data/company-stock-movements.csv\", index_col = 0)\n",
    "movements.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，每一列是一家公司，每一行是時間點，值是股價(大概做過一些調整了，所以有正有負，我們可以不用管他，就當是股價即可)\n",
    "* 如果我今天要做的分群，是對絕對的數值做分群，那我就直接用這張表分群就好. \n",
    "* 但如果我今天是想對 \"走勢\" 做分群，那我會希望對 \"列\" 做標準化。\n",
    "  * 舉例來說，台積電的股價變化是 600, 580, 600, 620, 640，啟基是 60, 58, 60, 62, 64。\n",
    "  * 那從 \"走勢\" 來看，台積跟啟基走勢是一樣的，應該被分為一群，但如果直接做 kmeans，就再見了，因為光 600 和 60 的距離就很遠。  \n",
    "  * 另外，台積股價的變化是 -20, 20, 20, 20; 啟基是 -2, 2, 2, 2，這個變動差距也不同，但如果改成看變化百分比(把股價放分母，變化當分子)，那兩邊就又差不多了. \n",
    "  * 所以，我如果先對列做標準化，那兩個公司的數值就都變成 [-0.39, -1.37, -0.39, 0.58, 1.56]，一模模一樣樣，euclidean distance 變成 0，分群時一定放在一塊兒\n",
    "* 所以，這一個例子，我們就要對 \"列\" 做標準化，那就要用到 `Normalizer` 這個 preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('normalizer', Normalizer()),\n",
       "                ('kmeans', KMeans(n_clusters=10))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# Create a normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a KMeans model with 10 clusters: kmeans\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
    "pipeline = make_pipeline(normalizer, kmeans)\n",
    "\n",
    "# Fit pipeline to the daily price movements\n",
    "pipeline.fit(movements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>companies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>Home Depot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>Lookheed Martin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>Northrop Grumman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Boeing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2</td>\n",
       "      <td>Xerox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>General Electrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>Microsoft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3</td>\n",
       "      <td>Texas instruments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>Taiwan Semiconductor Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>Symantec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>Navistar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>3M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>McDonalds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>MasterCard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>Intel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>IBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>American express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3</td>\n",
       "      <td>Yahoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>Caterpillar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>Google/Alphabet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>Cisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>DuPont de Nemours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>Dell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>Canon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>Sony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>Toyota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>Ford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>Mitsubishi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>Honda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>HP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Colgate-Palmolive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>Coca Cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5</td>\n",
       "      <td>Pepsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6</td>\n",
       "      <td>Unilever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6</td>\n",
       "      <td>Sanofi-Aventis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6</td>\n",
       "      <td>SAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6</td>\n",
       "      <td>Royal Dutch Shell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>GlaxoSmithKline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>British American Tobacco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6</td>\n",
       "      <td>Novartis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6</td>\n",
       "      <td>Total</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>7</td>\n",
       "      <td>Exxon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7</td>\n",
       "      <td>Philip Morris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>7</td>\n",
       "      <td>Schlumberger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>Chevron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>7</td>\n",
       "      <td>Valero Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "      <td>ConocoPhillips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>8</td>\n",
       "      <td>Wells Fargo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>Goldman Sachs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>AIG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>JPMorgan Chase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>Bank of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9</td>\n",
       "      <td>Kimberly-Clark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>9</td>\n",
       "      <td>Walgreen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9</td>\n",
       "      <td>Johnson &amp; Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>9</td>\n",
       "      <td>Wal-Mart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9</td>\n",
       "      <td>Procter Gamble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9</td>\n",
       "      <td>Pfizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    labels                           companies\n",
       "20       0                          Home Depot\n",
       "29       1                     Lookheed Martin\n",
       "36       1                    Northrop Grumman\n",
       "4        1                              Boeing\n",
       "58       2                               Xerox\n",
       "16       2                   General Electrics\n",
       "0        2                               Apple\n",
       "33       3                           Microsoft\n",
       "51       3                   Texas instruments\n",
       "50       3  Taiwan Semiconductor Manufacturing\n",
       "47       3                            Symantec\n",
       "35       3                            Navistar\n",
       "32       3                                  3M\n",
       "31       3                           McDonalds\n",
       "30       3                          MasterCard\n",
       "2        3                              Amazon\n",
       "24       3                               Intel\n",
       "23       3                                 IBM\n",
       "3        3                    American express\n",
       "59       3                               Yahoo\n",
       "8        3                         Caterpillar\n",
       "17       3                     Google/Alphabet\n",
       "11       3                               Cisco\n",
       "13       3                   DuPont de Nemours\n",
       "14       3                                Dell\n",
       "7        4                               Canon\n",
       "45       4                                Sony\n",
       "48       4                              Toyota\n",
       "15       4                                Ford\n",
       "34       4                          Mitsubishi\n",
       "21       4                               Honda\n",
       "22       4                                  HP\n",
       "9        5                   Colgate-Palmolive\n",
       "28       5                           Coca Cola\n",
       "38       5                               Pepsi\n",
       "52       6                            Unilever\n",
       "46       6                      Sanofi-Aventis\n",
       "43       6                                 SAP\n",
       "42       6                   Royal Dutch Shell\n",
       "19       6                     GlaxoSmithKline\n",
       "6        6            British American Tobacco\n",
       "37       6                            Novartis\n",
       "49       6                               Total\n",
       "57       7                               Exxon\n",
       "41       7                       Philip Morris\n",
       "44       7                        Schlumberger\n",
       "12       7                             Chevron\n",
       "53       7                       Valero Energy\n",
       "10       7                      ConocoPhillips\n",
       "55       8                         Wells Fargo\n",
       "18       8                       Goldman Sachs\n",
       "1        8                                 AIG\n",
       "26       8                      JPMorgan Chase\n",
       "5        8                     Bank of America\n",
       "27       9                      Kimberly-Clark\n",
       "54       9                            Walgreen\n",
       "25       9                   Johnson & Johnson\n",
       "56       9                            Wal-Mart\n",
       "40       9                      Procter Gamble\n",
       "39       9                              Pfizer"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the cluster labels: labels\n",
    "labels = pipeline.predict(movements)\n",
    "\n",
    "# Create a DataFrame aligning labels and companies: df\n",
    "df = pd.DataFrame({'labels': labels, \n",
    "                   'companies': movements.index})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "df.sort_values([\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ds_env",
   "language": "python",
   "name": "python_ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
